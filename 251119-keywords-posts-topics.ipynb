{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bc03ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from db.database import Database\n",
    "from db.models import Blueprint\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "stopwords = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c45bbecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "de0ffb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    for code_tag in soup.find_all('code', class_='lang-auto'):\n",
    "        code_tag.decompose()\n",
    "    \n",
    "    for a_tag in soup.find_all('a'):\n",
    "        a_tag.decompose()\n",
    "    \n",
    "    return soup.get_text().replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "018f9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text = remove_html(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = text.lower()\n",
    "    text = text.split()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    text = ' '.join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d165d607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_trans(str: list[str]):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    res = tfidf.fit_transform(str)\n",
    "    return res, tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0abc23db",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bool' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m posts = db.get_posts_by_topic_id(topics[\u001b[32m100\u001b[39m].topic_id)\n\u001b[32m      3\u001b[39m post_contents = [post.cooked \u001b[38;5;28;01mfor\u001b[39;00m post \u001b[38;5;129;01min\u001b[39;00m posts]\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m corpus = [\u001b[43mpreprocessing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m post_contents]\n\u001b[32m      5\u001b[39m corpus\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mpreprocessing\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      4\u001b[39m text = text.lower()\n\u001b[32m      5\u001b[39m text = text.split()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m text = [lemmatizer.lemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m text \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mstopwords\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwords\u001b[49m(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m)]\n\u001b[32m      7\u001b[39m text = \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(text)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mAttributeError\u001b[39m: 'bool' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "topics = db.get_topics()\n",
    "posts = db.get_posts_by_topic_id(topics[100].topic_id)\n",
    "post_contents = [post.cooked for post in posts]\n",
    "corpus = [preprocessing(text) for text in post_contents]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd9236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ayselaydin.medium.com/5-tf-idf-a-traditional-approach-to-feature-extraction-in-nlp-using-python-348873750e1e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0175c37",
   "metadata": {},
   "source": [
    "### Testing Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "78c59e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stage trigger monitor', 9.0),\n",
       " ('state trigger monitors', 8.0),\n",
       " ('sensor’s action attribute', 6.833333333333334),\n",
       " ('action attribute', 4.5),\n",
       " ('sensor’s state', 4.333333333333334),\n",
       " ('short press', 4.0),\n",
       " ('long press', 4.0),\n",
       " ('works great', 4.0),\n",
       " ('z2m binding', 4.0),\n",
       " ('brightness level', 4.0),\n",
       " ('can’t answer', 4.0),\n",
       " ('ikea switch', 4.0),\n",
       " ('variable refers', 4.0),\n",
       " ('hobby programmer', 4.0),\n",
       " ('made product', 4.0),\n",
       " ('on/off switch', 3.666666666666667),\n",
       " ('realy don’t', 3.5),\n",
       " ('on/off function', 3.166666666666667),\n",
       " ('state', 2.0),\n",
       " ('on/off', 1.6666666666666667),\n",
       " ('2 function', 1.5),\n",
       " ('don’t', 1.5),\n",
       " ('based', 1.0),\n",
       " ('it’s', 1.0),\n",
       " ('code', 1.0),\n",
       " ('control', 1.0),\n",
       " ('lights', 1.0),\n",
       " ('dim', 1.0),\n",
       " ('bunch', 1.0),\n",
       " ('bad', 1.0),\n",
       " ('maybee', 1.0),\n",
       " ('rewrite', 1.0),\n",
       " ('dimming', 1.0),\n",
       " ('examples', 1.0),\n",
       " ('question', 1.0),\n",
       " ('zigbee2mqtt', 1.0),\n",
       " ('i’m', 1.0),\n",
       " ('rebuild', 1.0)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multi_rake import Rake\n",
    "rake = Rake()\n",
    "full_text = \" \".join(cleaned_texts_topic)\n",
    "keywords = rake.apply(full_text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b345ee26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpclassification",
   "language": "python",
   "name": "bpclassification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
